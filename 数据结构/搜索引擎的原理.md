# 网页来源

- 搜索引擎网页的来源：一个爬虫递归爬取网页的url，将爬到的url保存下来，形成自己的网页数据库

- 网页存储判重：使用布隆过滤器

  10亿个url分配100亿个比特，存储大小为1.2G，相比用散列表查询（100G）节省空间。

- 网页存储：10亿个网页，用10亿个文件不现实，文件系统支持不了，所以一个文件要存储多个网页内容。

  doc_raw.bin

```
doc_id doc_size doc_content
网页id 网页大小 网页内容
```

​	文件系统对于单个文件的大小也是有限制的，比如1G，存满1G换文件存。

- 网页id的产生，用发号机分配id，将id和url存储到一个文件中，例如doc_id.bin

```
doc_id url
网页id 网页url
```

# 预处理

​	删减网页html代码,去除脚本 格式等无用代码,只留下文本信息。

- 用到的字符串匹配算法：BF，KMP单模式串匹配算法。一次性查询出多个匹配字段，使用AC自动机多模式串匹配算法
- 处理完无用标签及内容后，去除留下的html标签，保留文本内容，最终需要的是纯内容

# 分词并创建倒排索引

英文分词，按照空格区分单词。

中文分词，按照词库中的词语进行匹配。

按照分词，把具有相同的分词的网页归为一类。

搜索引擎搜索时，把分词的结果返回。

搜索结果展示涉及到PageRank页面排序算法和TF-IDF算法

搜索提示语的实现，涉及到trie树，字典树

热门搜索字符串 实现

读取用户的搜索日志，日志中的关键词组成一颗trie数

对trie进行改造，把构成一个单词的节点的标识位改成搜索的次数

然后创建一颗小顶堆，把节点（关键词和搜索次数）传递给小顶堆，最后剩下的节点即为最热门的搜索字符串

